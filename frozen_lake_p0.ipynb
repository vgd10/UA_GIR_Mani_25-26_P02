{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc233d8-6e79-4ec7-b24a-7cd06efa5333",
   "metadata": {},
   "source": [
    "# Práctica 0: Frozen Lake\n",
    "\n",
    "En este notebook trabajaremos con el entorno **Frozen Lake**, parte de la librería OpenAI Gym. Este entorno simula un lago congelado donde un agente debe aprender a llegar a su objetivo (la meta) mientras evita caer en agujeros en el hielo. Es un problema clásico de **aprendizaje por refuerzo** (Reinforcement Learning) que ilustra conceptos como:\n",
    "\n",
    "- Modelado de entornos utilizando **procesos de decisión de Markov (MDPs)**.\n",
    "- Definición de políticas y recompensas.\n",
    "- Uso de algoritmos de planificación como **Value Iteration** y **Policy Iteration**.\n",
    "\n",
    "## Descripción del entorno\n",
    "El entorno de Frozen Lake consiste en una cuadrícula de 4x4 donde cada celda puede representar uno de los siguientes estados:\n",
    "- **S** (Start): La posición inicial del agente.\n",
    "- **F** (Frozen): Una celda segura que el agente puede atravesar.\n",
    "- **H** (Hole): Un agujero en el que el agente caerá, terminando el episodio.\n",
    "- **G** (Goal): El objetivo que el agente debe alcanzar.\n",
    "\n",
    "![Escenario](https://www.gymlibrary.dev/_images/frozen_lake.gif)\n",
    "\n",
    "El agente puede moverse (acciones) en cuatro direcciones: **arriba**, **abajo**, **izquierda**, y **derecha**. Aunque el ejemplo clásico incluye el efecto de hielo resbaladizo, lo que introduce un grado de aleatoriedad en el entorno, en este ejercicio y con el objetivo de centrarnos en otros aspectos, utilizaremos la versión sin Sin embargo, debido al hielo resbaladizo, los movimientos no siempre son precisos, lo que introduce un grado de aleatoriedad en el entorno. Sin embargo, por simplificar el ejercicio, no incluiremos la parte resbaladiza.\n",
    "\n",
    "En caso de que alguno de los acciones haga que el agente salga del mapa, la acción no se llevará a cabo (y su estado no cambiará)\n",
    "\n",
    "### Acciones\n",
    "\n",
    "El agente utilizará un vector de 1 elemento para las acciones que puede realizar. En el ejercicio de **FrozenLake**, el espacio de acciones es discreto, y este puede tomar uno de los siguientes valores para decidir la dirección en la que moverse:\n",
    "\n",
    "- 0: IZQUIERDA\n",
    "\n",
    "- 1: ABAJO\n",
    "\n",
    "- 2: DERECHA\n",
    "\n",
    "- 3: ARRIBA\n",
    "\n",
    "### Estados (o espacio de observación)\n",
    "\n",
    "El modelado de estados es una parte esencial a la hora de resolver los problemas siguiendo el proceso de decisión de Markov (MDP). El estado representa la posición actual del agente. El número de estados posibles depende del mapa en este caso. Dado que tenemos un problema discreto, donde los estados posibles son 16 (uno para cada posición en la rejilla 4x4), el estado (o posición actual) del agente puede calcularse como *fila actual x n_filas + columna actual*, donde tanto fila como columna empiezan en 0. Por ejemplo, la posicion inicial será el estado 0 (*estado = 0 x 4 + 0 = 0*). El estado final puede calcularse de la siguiente manera (*estado = 3 x 4 + 3 = 15*). O el bloque de hielo de la ultima fila seria (*estado = 3 x 4 + 0 = 12*) \n",
    "\n",
    "\n",
    "## Objetivo\n",
    "El objetivo del ejercicio es implementar diferentes estrategias de aprendizaje para entrenar al agente a maximizar su probabilidad de alcanzar la meta sin caer en los agujeros.\n",
    "\n",
    "Más en detalle, vamos a implementar diferentes politicas y algoritmos de aprendizaje por refuerzo.\n",
    "\n",
    "1. **Montecarlo**\n",
    "2. **Q-Learning**\n",
    "\n",
    "Además, utilizaremos para cada uno de los algoritmos diferentes politicas:\n",
    "\n",
    "1. **Greedy**\n",
    "2. **Epsilon-Greedy**\n",
    "3. **SOFTMAX**\n",
    "\n",
    "### Herramientas necesarias\n",
    "Este notebook utilizará:\n",
    "- **Python**: Para implementar el código.\n",
    "- **OpenAI Gym**: Para simular el entorno.\n",
    "- **Numpy**: Para cálculos numéricos.\n",
    "- **Matplotlib** (opcional): En caso de que quieras visualizar los resultados con más detalle.\n",
    "\n",
    "Para simplificar la configuración, y evitar problemas de versiones y configuraciones, tienes en el PDF adjunto de la práctica los pasos para instalar las librerias necesarias. \n",
    "\n",
    "Ahora si, con todo listo y sabiendo que tenemos que implementar, ¡comencemos a explorar Frozen Lake!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a286a5",
   "metadata": {},
   "source": [
    "Lo primero es importar, las librerias que vamos a utilizar. El entorno gym ya importa algunos paquetes cuando lo invocamos (como pygame) que es lo que nos permite visualizar el agente y el entorno. En este sentido, será suficiente con que importemos las librerias de Gymnasium y Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6968dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import RewardWrapper\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85687c",
   "metadata": {},
   "source": [
    "# Creación del entorno en Gym\n",
    "Ahora vamos a crear el entorno haciendo uso de la función **gym.make()** \n",
    "\n",
    "[Documentación de frozen lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "- En el primer argumento del método (**id del juego**) le indicaremos el entorno que vamos a cargar, en este caso sera *FrozenLake-v1*. Si quisiesemos probar en otros entornos (aunque no es objetivo de la práctica), cambiariamos la versión o el nombre en este parámetro.\n",
    "\n",
    "- El segundo argumento **desc**, no lo vamos a utilizar (por simplicidad), pero basicamente nos permitiría definir nuestro propio entorno de obstaculos y bloques de hielo en lugar del entorno por defecto que vamos a utilizar.\n",
    "\n",
    "- Otro parametro importante es **map_name**, y depende del entorno. En el caso del juego *FrozenLake*, este puede obtener los valore \"4x4\" o \"8x8\". En nuestro caso, elegiremos el entorno 4x4, para facilitar la convergencia de los algoritmos.\n",
    "\n",
    "- **is_slippery** es un parámetro especifico del juego FrozenLake. Si el valor de este parámetro fuese True, lo que haría es aplicar un factor de \"resbalamiento\" o probabilidad de resbalar, que con una probabilidad 1/3 repetiria la acción anterior (simulando que el agente resbala y mantiene la dirección). Tal y como indicabamos previamente, no vamos a incluir este factor de aleatoriedad y pondremos el valor a False. Se usa en conjunción con el parámetro **success_rate** para indicar la probabilidad de resbalar en un bloque de hielo.\n",
    "\n",
    "- **reward_schedule** es el parámetro que permite modificar las recompensas obtenidas por el entorno. Como podras ver en la documentación, por defecto recibe (1, 0, 0), que son las recompensas que recibe el agente cuando alcanza la emta, alcanza un agujero, o alcanza un bloque de hielo respectivamente.\n",
    "\n",
    "- El último parámetro, que no aparece en la documentación es **render_mode**. Aunque se podria visualizar mediante una matriz que indique el estado (realmente se representa así), vamos a utilizar el modo \"human\" el cual usa pygame, y nos permitira ver el comportamiento del agente por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff256cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el entorno FrozenLake indicandole los parámetros. Para empezar, que el parámetro is_slippery = False.\n",
    "env = gym.make('FrozenLake-v1',\n",
    "               desc=None,\n",
    "               map_name=\"4x4\",\n",
    "               is_slippery=False,\n",
    "               reward_schedule=(1, 0, 0),\n",
    "               render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb508efe",
   "metadata": {},
   "source": [
    "# Ejecución simple del entorno FrozenLake\n",
    "\n",
    "Antes de empezar a programar políticas y algoritmos, vamos a comprobar que todo nos funciona correctamente. Para ello, vamos a ejecutar el entorno realizando movimientos aleatorios. Además, puedes programar tambien tu propio episodio donde el agente realice las acciones predeterminadas que determine.\n",
    "\n",
    "En el siguiente [Enlace](https://gymnasium.farama.org/api/env/) puedes ver los métodos más importantes para interactuar con el entorno. A modo de resumen, el **método reset** nos permite reiniciar el entorno entre diferentes episodios(ya que tras un episodio queremos volver a comenzar en la posición inicial). El **método render**, ya lo utilizamos al principio, pero nos permite visualizar lo que el agente hace (en este caso sera human ya que lo definimos previamente) aunque más adelante usaremos otros modos. Y por último, el método más importante es el método **step** ya que es el método que nos va a permitir la transicción de estado (dada una acción).\n",
    "\n",
    "Los pseudopasos a grandes rasgos (aunque te ayudo con los comentarios en la siguiente celda) son:\n",
    "\n",
    "1. Define el número de episodios\n",
    "2. Para cada episodio debemos de realizar una serie de pasos:\n",
    "   - Reinicia el entorno mediante el método reset.\n",
    "   - Mientras no hayamos alcanzado un estado terminal (esto se produce cuando alcanza la meta, o cae en un bloque de hielo)... \n",
    "        - Renderiza\n",
    "        - Elige una acción (en este caso elegiremos una acción aleatoria o predeterminada). Para el caso de la acción aleatoria, es preferible definir un método que realice dicha acción. En un futuro cambiaremos la politica ;) \n",
    "        - Ejecuta un step (ten en cuenta las variables que devuelve para que te funcione ;) )\n",
    "3. Recuerda cerrar el entorno mediante el **método env.close()** tras finalizar TODOS los episodios\n",
    "\n",
    "Tras implementar estos pasos correctamente, veras el tablero y como el agente interactua con el entorno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f561411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mani/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 3\n",
      "0 4 1\n",
      "0 0 3\n",
      "0 4 1\n",
      "0 8 1\n",
      "0 12 1\n",
      "1 0 3\n",
      "1 0 3\n",
      "1 1 2\n",
      "1 5 1\n"
     ]
    }
   ],
   "source": [
    "# Variables para almacenar la información de cada episodio\n",
    "\n",
    "\n",
    "#Define el número de episodios (2 es un buen número para empezar)\n",
    "episodes = 2\n",
    "for episode in range(episodes):\n",
    "    # Reiniciar el entorno\n",
    "    act, _ =env.reset()\n",
    "\n",
    "    # Simular un episodio hasta que llege.\n",
    "    terminated = False\n",
    "    while not (terminated):\n",
    "\n",
    "        # Elegir una acción aleatoria\n",
    "        act = env.action_space.sample()\n",
    "        \n",
    "        #Renderización del entorno para visualizar lo que el agente hace\n",
    "        env.render()\n",
    "\n",
    "        #Ejecutamos un step\n",
    "        observation,reward,terminated,truncated,_ = env.step(act)\n",
    "        \n",
    "        # Actualizar el estado\n",
    "        print(episode,observation,act)\n",
    "        \n",
    "#Cerramos el entorno\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15aea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMAINDER: inputs mode dictates on which way parameters are expected\n",
      "0 == Only 1 value expected // 1 == 3 values expected: A maximum, minimum and bins to space them)  //  >1 == A costum amount of values equal to the mode\n",
      "\n",
      "Set learning rate input mode [MIN: 0, MAX: 5, TYPE: int]: 1\n",
      "Set learning rate (maximum) [MIN: 0.3, MAX: 0.9, TYPE: float]: 0.9\n",
      "Set learning rate (minimum) [MIN: 0.3, MAX: 0.9, TYPE: float]: 0.6\n",
      "Set learning rate (bins) [MIN: 1, MAX: 10, TYPE: int]: 6\n",
      "Set the discount reward input mode [MIN: 0, MAX: 5, TYPE: int]: 1\n",
      "Set the discount reward (maximum) [MIN: 0.8, MAX: 0.95, TYPE: float]: 0.9\n",
      "Set the discount reward (minimum) [MIN: 0.8, MAX: 0.95, TYPE: float]: 0.8\n",
      "Set the discount reward (bins) [MIN: 1, MAX: 10, TYPE: int]: 3\n",
      "Set environment to use (FrozenLake-v1_4x4_noslip: 0 // LunarLander-v3: 1) [MIN: 0, MAX: 1, TYPE: int]: 0\n",
      "Set algorithm to train (Q-learning: 1 // SARSA: 2) [MIN: 1, MAX: 2, TYPE: int]: 1\n",
      "Set policy to use (Epsilon-greedy: 0 // Softmax: 1) [MIN: 0, MAX: 1, TYPE: int]: 0\n",
      "Set a starting epsilon [MIN: 0.1, MAX: 0.999, TYPE: float]: 0.999\n",
      "Set a minimum epsilon [MIN: 0.01, MAX: 0.001, TYPE: float]: 0.01\n",
      "Set a decay epsilon [MIN: 0.9, MAX: 0.999, TYPE: float]: 0.95\n",
      "Set max episodes input mode [MIN: 0, MAX: 5, TYPE: int]: 0\n",
      "Set max episodes [MIN: 20, MAX: 1000, TYPE: int]: 100\n",
      "Set a window size (used to calculate mean of accumulated rewards) [MIN: 10, MAX: 200, TYPE: int]: 10\n"
     ]
    }
   ],
   "source": [
    "# Bounds to ask for parameters\n",
    "USERMODE_BOUND = [0,5,0]\n",
    "\n",
    "ALPHA_BOUND = [0.3,0.9,1]\n",
    "\n",
    "SPACE_BOUND = [1,10,0]\n",
    "\n",
    "\n",
    "GAMMA_BOUND = [0.8,0.95,1]\n",
    "\n",
    "POLICY_BOUND = [0,1,0]\n",
    "ALGORITHM_BOUND = [1,2,0]\n",
    "ENVIRONMENT_BOUND = [0,1,0]\n",
    "\n",
    "VPOLICY_BOUND = [0.1,0.999,1]\n",
    "MINVPOLICY_BOUND = [0.01,0.001,1]\n",
    "DECAYVPOLICY_BOUND = [0.9,0.999,1]\n",
    "\n",
    "EPISODESENV0_BOUND = [20,1000,0]\n",
    "EPISODESENV1_BOUND = [500,5000,0]\n",
    "\n",
    "WINDOW_BOUND = [10,200,0]\n",
    "\n",
    "def inputBoundedParameter(text,bound):\n",
    "    datatype = \"int\"\n",
    "    if bound[2] > 0:\n",
    "        datatype = \"float\"\n",
    "\n",
    "    value = input(text+f\" [MIN: {bound[0]}, MAX: {bound[1]}, TYPE: {datatype}]: \")\n",
    "    if bound[2] < 1:\n",
    "        return min(max(int(value),bound[0]),bound[1])\n",
    "    return min(max(float(value),bound[0]),bound[1])\n",
    "\n",
    "\n",
    "def inputParameter(text,bounds):\n",
    "    values = []\n",
    "    v_parameters = []\n",
    "\n",
    "    mode = inputBoundedParameter(text+\" input mode\",USERMODE_BOUND)\n",
    "\n",
    "    if len(bounds) > 0:\n",
    "        if mode < 1:\n",
    "            values.append(inputBoundedParameter(text,bounds[0]))\n",
    "        elif mode < 2 and len(bounds) > 2:\n",
    "            mod = [\"maximum\",\"minimum\",\"bins\"]\n",
    "            for i in range(3):\n",
    "                v_parameters.append(inputBoundedParameter(text+\" (\"+mod[i]+\")\",bounds[i]))\n",
    "        else:\n",
    "            for j in range(mode):\n",
    "                values.append(inputBoundedParameter(text+f\" ({j})\",bounds[0]))\n",
    "    \n",
    "    if len(v_parameters) > 0:\n",
    "        if len(v_parameters) > 2 and v_parameters[0] > v_parameters[1]:\n",
    "            values = np.linspace(v_parameters[0],v_parameters[1],v_parameters[2])\n",
    "        else:\n",
    "            values.append(v_parameters[0])\n",
    "\n",
    "    return values,mode\n",
    "\n",
    "\n",
    "\n",
    "print(\"REMAINDER: inputs mode dictates on which way parameters are expected\")\n",
    "print(\"0 == Only 1 value expected // 1 == 3 values expected: A maximum, minimum and bins to space them)  //  >1 == A costum amount of values equal to the mode\\n\")\n",
    "\n",
    "# Intrinsic RL parameters\n",
    "# usermode = inputBoundedParameter(\"Set user mode (Manual: 0 // Recover: 1 // Automatic: 2)\",USERMODE_BOUND)\n",
    "\n",
    "alphas,_ = inputParameter(\"Set learning rate\",[ALPHA_BOUND,ALPHA_BOUND,SPACE_BOUND])        # Learning rate parameters\n",
    "gammas,_ = inputParameter(\"Set the discount reward\",[GAMMA_BOUND,GAMMA_BOUND,SPACE_BOUND])  # Discount parameter\n",
    "\n",
    "environment = inputBoundedParameter(\"Set environment to use (FrozenLake-v1_4x4_noslip: 0 // LunarLander-v3: 1)\",ENVIRONMENT_BOUND)\n",
    "algorithm = inputBoundedParameter(\"Set algorithm to train (Q-learning: 1 // SARSA: 2)\",ALGORITHM_BOUND)\n",
    "policy = inputBoundedParameter(\"Set policy to use (Epsilon-greedy: 0 // Softmax: 1)\",POLICY_BOUND)\n",
    "\n",
    "if algorithm < 2:\n",
    "    s1 = \"starting\"\n",
    "else:\n",
    "    s1 = \"permanent\"\n",
    "\n",
    "if policy < 1:\n",
    "    s2 = \"epsilon\"\n",
    "else:\n",
    "    s2 = \"temperature\"\n",
    "\n",
    "v_policies = []\n",
    "\n",
    "v_policies.append([])\n",
    "v_policies[-1].append(inputBoundedParameter(f\"Set a {s1} {s2}\",VPOLICY_BOUND))\n",
    "if algorithm < 2:\n",
    "    v_policies[-1].append(inputBoundedParameter(f\"Set a minimum {s2}\",MINVPOLICY_BOUND))\n",
    "    v_policies[-1].append(inputBoundedParameter(f\"Set a decay {s2}\",DECAYVPOLICY_BOUND))\n",
    "\n",
    "# Total episodes of the training\n",
    "if environment < 1:\n",
    "    envname = \"FrozenLake-v1\"\n",
    "    max_episodes,_ = inputParameter(\"Set max episodes\",[EPISODESENV0_BOUND,EPISODESENV0_BOUND,SPACE_BOUND])\n",
    "else:\n",
    "    envname = \"LunarLander-v3\"\n",
    "    max_episodes,_ = inputParameter(\"Set max episodes\",[EPISODESENV1_BOUND,EPISODESENV1_BOUND,SPACE_BOUND])\n",
    "    \n",
    "window = inputBoundedParameter(\"Set a window size (used to calculate mean of accumulated rewards)\",WINDOW_BOUND) \n",
    "\n",
    "if policy < 1:\n",
    "    epsilon = v_policies[-1][0]\n",
    "    if algorithm < 2:\n",
    "        min_epsilon = v_policies[-1][1]\n",
    "        decay_epsilon = v_policies[-1][2]\n",
    "else:\n",
    "    temperature = v_policies[-1][0]\n",
    "    if algorithm < 2:\n",
    "        min_temperature = v_policies[-1][1]\n",
    "        decay_temperature = v_policies[-1][2]\n",
    "\n",
    "# Action policies\n",
    "\n",
    "def epsilon_greedy(state,epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "def softmax(state, temperature):\n",
    "    q_values = Q[state]\n",
    "    # Evitar overflow: restar el max\n",
    "    z = (q_values - np.max(q_values)) / max(temperature, 1e-6)\n",
    "    probs = np.exp(z)\n",
    "    probs /= np.sum(probs)\n",
    "    return int(np.random.choice(len(q_values), p=probs))\n",
    "\n",
    "\n",
    "\n",
    "def select_action(state,policy,parameter_policy):\n",
    "    if policy < 1:\n",
    "        return epsilon_greedy(state,parameter_policy)\n",
    "    return softmax(state,parameter_policy)\n",
    "\n",
    "\n",
    "\n",
    "# =========== LUNAR-LANDER ONLY ===========\n",
    "\n",
    "bins = 12 # Amount of subdivisions space is divided\n",
    "\n",
    "# State discretization values -> x,y,vx,vy,angle,w,contactleg1,contactleg2\n",
    "state_bins = [\n",
    "    np.linspace(-1.2, 1.2, bins),\n",
    "    np.linspace(-1.2, 1.2, bins),\n",
    "    np.linspace(-2.0, 2.0, bins),\n",
    "    np.linspace(-2.0, 2.0, bins),\n",
    "    np.linspace(-3.14, 3.14, bins),\n",
    "    np.linspace(-5.0, 5.0, bins),\n",
    "    np.array([0, 1]),\n",
    "    np.array([0, 1])\n",
    "]\n",
    "\n",
    "\n",
    "# State discretization function\n",
    "def discretize(state):\n",
    "    disc = []\n",
    "    for i in range(len(state)):\n",
    "        if i < 6:\n",
    "            disc.append(int(np.digitize(state[i], state_bins[i])))\n",
    "        else:\n",
    "            disc.append(int(state[i]))\n",
    "    return tuple(disc)\n",
    "\n",
    "target_mean_reward = 200  # Standard objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cea9dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m start_time = time.time()\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[43mepisodes\u001b[49m+\u001b[32m1\u001b[39m):\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Environment reset\u001b[39;00m\n\u001b[32m     55\u001b[39m     state, _ =env.reset()\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Prepare store data for this episode (epsilon only used for Q-learning + ep_greedy)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'episodes' is not defined"
     ]
    }
   ],
   "source": [
    "# ================ Lunar-Lander ================\n",
    "\n",
    "# Storing variables\n",
    "\n",
    "# Action, reward and next state matrix [i][j] (i -> Episode // j -> Step on that episode)\n",
    "ev_act = []\n",
    "ev_reward = []\n",
    "ev_state = []\n",
    "\n",
    "# Epsilon and accumulated reward matrix (length = episodes)\n",
    "ev_epsilon = []\n",
    "acc_reward = []\n",
    "\n",
    "best_mean = -np.inf\n",
    "\n",
    "# Environment initialization\n",
    "if environment < 1:\n",
    "    env = gym.make(envname,desc=None,map_name=\"4x4\",is_slippery=False,reward_schedule=(10, -1, -1),render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make(envname,render_mode=None)\n",
    "\n",
    "# Q-table first initialization\n",
    "if environment < 1:\n",
    "    Q = (-1)*np.ones([env.observation_space.n, env.action_space.n])\n",
    "else:\n",
    "    Q = np.zeros([bins+1] * 6 + [2, 2] + [env.action_space.n])\n",
    "\n",
    "best_Q = None\n",
    "\n",
    "# Epsilon-greedy parameters\n",
    "epsilon = 0.999       # Starting (Q-learning only) epsilon value\n",
    "min_epsilon = 0.05    # Minimum epsilon value achievable\n",
    "decay_epsilon = 0.995 # Decay parameter for epsilon greedy (enacted each episode once goal is reached)\n",
    "\n",
    "# Softmax parameters\n",
    "temperature = 0.999         # Starting (Q-learning only) temperature value\n",
    "min_temperature = 0.1       # Minimum temperature value achievable\n",
    "decay_temperature = 0.995   # Decay parameter for softmax (enacted each episode once goal is reached)\n",
    "\n",
    "\n",
    "if environment < 1:\n",
    "    max_steps = 20  # Maximum steps allowed before terminating episode no matter what\n",
    "\n",
    "mean_recent = -200\n",
    "\n",
    "# Flag and episode number when objective is reached for first time\n",
    "objective_reached = False\n",
    "ep_ob_reached = -1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training phase\n",
    "for episode in range(1,episodes+1):\n",
    "    # Environment reset\n",
    "    state, _ =env.reset()\n",
    "\n",
    "    # Prepare store data for this episode (epsilon only used for Q-learning + ep_greedy)\n",
    "    if environment < 1:\n",
    "        ev_state.append([])\n",
    "        ev_act.append([])\n",
    "        ev_reward.append([])\n",
    "        steps = 0\n",
    "    else:\n",
    "        state = discretize(state)\n",
    "\n",
    "    ev_epsilon.append(epsilon)\n",
    "    acc_reward.append(0.0)\n",
    "\n",
    "    # Set termination flags\n",
    "    terminated = False\n",
    "    truncated = False  # NOT USED\n",
    "    \n",
    "    if algorithm >= 2:\n",
    "        #act = epsilon_greedy(state,epsilon)\n",
    "        act = select_action(state,policy,v_policy)\n",
    "    \n",
    "    \n",
    "    # Episode simulation\n",
    "    while not (terminated):\n",
    "\n",
    "\n",
    "        # Action calculation (not SARSA)\n",
    "        if algorithm < 2:\n",
    "            # act = epsilon_greedy(state,epsilon)\n",
    "            act = select_action(state,policy,v_policy)\n",
    "\n",
    "\n",
    "        # Step execution\n",
    "        next_state,reward,terminated,truncated,_ = env.step(act)\n",
    "        \n",
    "        if environment > 1:\n",
    "            next_state = discretize(next_state)\n",
    "        \n",
    "\n",
    "        # Next best action calculation (Motecarlo and Q-learning only)\n",
    "        if algorithm < 2:\n",
    "            next_act = np.argmax(Q[next_state])\n",
    "        # Next action calculation (SARSA)\n",
    "        else:\n",
    "            # next_act = epsilon_greedy(state)\n",
    "            next_act = select_action(state,policy,v_policy)\n",
    "\n",
    "\n",
    "        # Q-table and state update\n",
    "        Q[state][act] += alpha * (reward + gamma * Q[next_state][next_act] - Q[state][act])\n",
    "        state = next_state\n",
    "        \n",
    "        # Save all gathered step info\n",
    "        if environment < 1:\n",
    "            ev_act[episode-1].append(act)\n",
    "            ev_reward[episode-1].append(reward)\n",
    "            ev_state[episode-1].append(state)\n",
    "        acc_reward[episode-1] += reward\n",
    "\n",
    "\n",
    "        # Start to erode epsilon value once target is reached for first time (Q-learning+ep_greedy only)\n",
    "        if mean_recent >= -150.0 and not objective_reached:\n",
    "            objective_reached = True\n",
    "            ep_ob_reached = episode\n",
    "        \n",
    "        # End condition, so each episode doesn't take absurdly long\n",
    "        if environment < 1:\n",
    "            if steps >= max_steps:\n",
    "                terminated = True\n",
    "            steps += 1\n",
    "\n",
    "    if objective_reached and algorithm < 2:\n",
    "        epsilon = max(min_epsilon,epsilon*decay_epsilon)\n",
    "        temperature = max(min_temperature,temperature*decay_temperature)\n",
    "        \n",
    "    # Estadísticas y parada temprana por media móvil\n",
    "    if len(acc_reward) >= window:\n",
    "        mean_recent = np.mean(acc_reward[-window:])\n",
    "        if mean_recent > best_mean:\n",
    "            best_mean = mean_recent\n",
    "            best_Q = Q.copy()\n",
    "        if mean_recent >= target_mean_reward:\n",
    "            print(f\"Parada temprana en episodio {episode}: media {mean_recent:.2f}\")\n",
    "            break\n",
    "\n",
    "    if episode % window == 0:\n",
    "        mean_recent = np.mean(acc_reward[-min(len(acc_reward), window):])\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Episode {episode} | Reward {acc_reward[episode-1]:.1f} | Mean({min(len(acc_reward), window)}) \"\n",
    "            f\"{mean_recent:.1f} | eps {epsilon:.3f} | temp {temperature:.3f} | alpha {alpha:.3f} | t {elapsed/60:.1f}m\"\n",
    "        )\n",
    "\n",
    "# Guardar el mejor modelo disponible\n",
    "to_save = best_Q if best_Q is not None else Q\n",
    "with open(f\"{envname}_qtable.pkl\", \"wb\") as f:\n",
    "    pickle.dump(to_save, f)\n",
    "print(f\"\\nModelo guardado como {envname}_qtable.pkl\\n\")\n",
    "\n",
    "env.close() # Close environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31aeffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Reward -6.0 | Mean(10) -9.3 | v_pol 0.999 | alpha 0.900 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -5.3 | v_pol 0.598 | alpha 0.900 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -3.0 | v_pol 0.358 | alpha 0.900 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.8 | v_pol 0.214 | alpha 0.900 | t 0.0m\n",
      "Episode 50 | Reward -3.0 | Mean(10) -2.2 | v_pol 0.128 | alpha 0.900 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.077 | alpha 0.900 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.900 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.900 | t 0.0m\n",
      "Episode 90 | Reward -4.0 | Mean(10) -2.2 | v_pol 0.016 | alpha 0.900 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.900 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.900_g0.900_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -19.0 | Mean(10) -10.1 | v_pol 0.999 | alpha 0.900 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -4.1 | v_pol 0.598 | alpha 0.900 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -3.1 | v_pol 0.358 | alpha 0.900 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.214 | alpha 0.900 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.128 | alpha 0.900 | t 0.0m\n",
      "Episode 60 | Reward -3.0 | Mean(10) -2.5 | v_pol 0.077 | alpha 0.900 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.900 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.900 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.900 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.900 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.900_g0.850_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -3.0 | Mean(10) -6.4 | v_pol 0.999 | alpha 0.900 | t 0.0m\n",
      "Episode 20 | Reward -3.0 | Mean(10) -4.6 | v_pol 0.598 | alpha 0.900 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -3.6 | v_pol 0.358 | alpha 0.900 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -3.5 | v_pol 0.214 | alpha 0.900 | t 0.0m\n",
      "Episode 50 | Reward -3.0 | Mean(10) -2.6 | v_pol 0.128 | alpha 0.900 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.077 | alpha 0.900 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.900 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.2 | v_pol 0.028 | alpha 0.900 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.016 | alpha 0.900 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.900 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.900_g0.800_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -4.0 | Mean(10) -6.4 | v_pol 0.999 | alpha 0.840 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -4.5 | v_pol 0.598 | alpha 0.840 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -3.5 | v_pol 0.358 | alpha 0.840 | t 0.0m\n",
      "Episode 40 | Reward -4.0 | Mean(10) -3.4 | v_pol 0.214 | alpha 0.840 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.128 | alpha 0.840 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.077 | alpha 0.840 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.840 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.840 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.840 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.840 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.840_g0.900_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -3.0 | Mean(10) -6.1 | v_pol 0.999 | alpha 0.840 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -5.7 | v_pol 0.598 | alpha 0.840 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -2.9 | v_pol 0.358 | alpha 0.840 | t 0.0m\n",
      "Episode 40 | Reward -3.0 | Mean(10) -3.1 | v_pol 0.214 | alpha 0.840 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.128 | alpha 0.840 | t 0.0m\n",
      "Episode 60 | Reward -3.0 | Mean(10) -2.2 | v_pol 0.077 | alpha 0.840 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.840 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.840 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.840 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.010 | alpha 0.840 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.840_g0.850_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -9.0 | Mean(10) -8.3 | v_pol 0.999 | alpha 0.840 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -4.7 | v_pol 0.598 | alpha 0.840 | t 0.0m\n",
      "Episode 30 | Reward -8.0 | Mean(10) -5.0 | v_pol 0.358 | alpha 0.840 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.214 | alpha 0.840 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.128 | alpha 0.840 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.6 | v_pol 0.077 | alpha 0.840 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.046 | alpha 0.840 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.840 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.016 | alpha 0.840 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.840 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.840_g0.800_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -16.0 | Mean(10) -7.4 | v_pol 0.999 | alpha 0.780 | t 0.0m\n",
      "Episode 20 | Reward -10.0 | Mean(10) -5.8 | v_pol 0.598 | alpha 0.780 | t 0.0m\n",
      "Episode 30 | Reward -4.0 | Mean(10) -3.2 | v_pol 0.358 | alpha 0.780 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.214 | alpha 0.780 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.128 | alpha 0.780 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.2 | v_pol 0.077 | alpha 0.780 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.780 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.780 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.780 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.780 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.780_g0.900_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -4.0 | Mean(10) -8.2 | v_pol 0.999 | alpha 0.780 | t 0.0m\n",
      "Episode 20 | Reward -5.0 | Mean(10) -4.6 | v_pol 0.598 | alpha 0.780 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -3.5 | v_pol 0.358 | alpha 0.780 | t 0.0m\n",
      "Episode 40 | Reward -3.0 | Mean(10) -2.8 | v_pol 0.214 | alpha 0.780 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.128 | alpha 0.780 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.077 | alpha 0.780 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.780 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.028 | alpha 0.780 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.780 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.2 | v_pol 0.010 | alpha 0.780 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.780_g0.850_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -2.0 | Mean(10) -6.9 | v_pol 0.999 | alpha 0.780 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -5.2 | v_pol 0.598 | alpha 0.780 | t 0.0m\n",
      "Episode 30 | Reward -4.0 | Mean(10) -3.0 | v_pol 0.358 | alpha 0.780 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -3.2 | v_pol 0.214 | alpha 0.780 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.6 | v_pol 0.128 | alpha 0.780 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -3.0 | v_pol 0.077 | alpha 0.780 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.780 | t 0.0m\n",
      "Episode 80 | Reward -3.0 | Mean(10) -2.1 | v_pol 0.028 | alpha 0.780 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.016 | alpha 0.780 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.780 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.780_g0.800_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -11.0 | Mean(10) -6.3 | v_pol 0.999 | alpha 0.720 | t 0.0m\n",
      "Episode 20 | Reward -5.0 | Mean(10) -7.1 | v_pol 0.598 | alpha 0.720 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -2.8 | v_pol 0.358 | alpha 0.720 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.9 | v_pol 0.214 | alpha 0.720 | t 0.0m\n",
      "Episode 50 | Reward -4.0 | Mean(10) -2.3 | v_pol 0.128 | alpha 0.720 | t 0.0m\n",
      "Episode 60 | Reward -3.0 | Mean(10) -2.6 | v_pol 0.077 | alpha 0.720 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.720 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.720 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.720 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.720 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.720_g0.900_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -2.0 | Mean(10) -8.8 | v_pol 0.999 | alpha 0.720 | t 0.0m\n",
      "Episode 20 | Reward -2.0 | Mean(10) -5.6 | v_pol 0.598 | alpha 0.720 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -3.5 | v_pol 0.358 | alpha 0.720 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.214 | alpha 0.720 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.128 | alpha 0.720 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.077 | alpha 0.720 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.720 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.720 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.720 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.720 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.720_g0.850_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -3.0 | Mean(10) -7.3 | v_pol 0.999 | alpha 0.720 | t 0.0m\n",
      "Episode 20 | Reward -4.0 | Mean(10) -5.9 | v_pol 0.598 | alpha 0.720 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -3.1 | v_pol 0.358 | alpha 0.720 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -1.6 | v_pol 0.214 | alpha 0.720 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.128 | alpha 0.720 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.077 | alpha 0.720 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.720 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.720 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.720 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.720 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.720_g0.800_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -3.0 | Mean(10) -8.8 | v_pol 0.999 | alpha 0.660 | t 0.0m\n",
      "Episode 20 | Reward -5.0 | Mean(10) -6.1 | v_pol 0.598 | alpha 0.660 | t 0.0m\n",
      "Episode 30 | Reward -5.0 | Mean(10) -2.5 | v_pol 0.358 | alpha 0.660 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -3.6 | v_pol 0.214 | alpha 0.660 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.7 | v_pol 0.128 | alpha 0.660 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.077 | alpha 0.660 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.046 | alpha 0.660 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.660 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.660 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.660 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.660_g0.900_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -6.0 | Mean(10) -5.9 | v_pol 0.999 | alpha 0.660 | t 0.0m\n",
      "Episode 20 | Reward -6.0 | Mean(10) -5.4 | v_pol 0.598 | alpha 0.660 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -4.1 | v_pol 0.358 | alpha 0.660 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.5 | v_pol 0.214 | alpha 0.660 | t 0.0m\n",
      "Episode 50 | Reward -3.0 | Mean(10) -2.6 | v_pol 0.128 | alpha 0.660 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.077 | alpha 0.660 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.660 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.028 | alpha 0.660 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.016 | alpha 0.660 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.660 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.660_g0.850_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -7.0 | Mean(10) -8.5 | v_pol 0.999 | alpha 0.660 | t 0.0m\n",
      "Episode 20 | Reward -9.0 | Mean(10) -5.5 | v_pol 0.598 | alpha 0.660 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -2.6 | v_pol 0.358 | alpha 0.660 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.214 | alpha 0.660 | t 0.0m\n",
      "Episode 50 | Reward -5.0 | Mean(10) -2.4 | v_pol 0.128 | alpha 0.660 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.2 | v_pol 0.077 | alpha 0.660 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.046 | alpha 0.660 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.2 | v_pol 0.028 | alpha 0.660 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.016 | alpha 0.660 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.660 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.660_g0.800_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -14.0 | Mean(10) -7.1 | v_pol 0.999 | alpha 0.600 | t 0.0m\n",
      "Episode 20 | Reward -10.0 | Mean(10) -6.8 | v_pol 0.598 | alpha 0.600 | t 0.0m\n",
      "Episode 30 | Reward -2.0 | Mean(10) -3.3 | v_pol 0.358 | alpha 0.600 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.5 | v_pol 0.214 | alpha 0.600 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.128 | alpha 0.600 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.077 | alpha 0.600 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.046 | alpha 0.600 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.028 | alpha 0.600 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.600 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.600 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.600_g0.900_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -3.0 | Mean(10) -8.8 | v_pol 0.999 | alpha 0.600 | t 0.0m\n",
      "Episode 20 | Reward -8.0 | Mean(10) -6.1 | v_pol 0.598 | alpha 0.600 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -3.4 | v_pol 0.358 | alpha 0.600 | t 0.0m\n",
      "Episode 40 | Reward -3.0 | Mean(10) -2.9 | v_pol 0.214 | alpha 0.600 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.128 | alpha 0.600 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.077 | alpha 0.600 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.3 | v_pol 0.046 | alpha 0.600 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.7 | v_pol 0.028 | alpha 0.600 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.016 | alpha 0.600 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.600 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.600_g0.850_p0.010_e100_qtable.pkl\n",
      "\n",
      "Episode 10 | Reward -2.0 | Mean(10) -7.2 | v_pol 0.999 | alpha 0.600 | t 0.0m\n",
      "Episode 20 | Reward -3.0 | Mean(10) -7.0 | v_pol 0.598 | alpha 0.600 | t 0.0m\n",
      "Episode 30 | Reward -3.0 | Mean(10) -3.2 | v_pol 0.358 | alpha 0.600 | t 0.0m\n",
      "Episode 40 | Reward -2.0 | Mean(10) -2.4 | v_pol 0.214 | alpha 0.600 | t 0.0m\n",
      "Episode 50 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.128 | alpha 0.600 | t 0.0m\n",
      "Episode 60 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.077 | alpha 0.600 | t 0.0m\n",
      "Episode 70 | Reward -2.0 | Mean(10) -2.1 | v_pol 0.046 | alpha 0.600 | t 0.0m\n",
      "Episode 80 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.028 | alpha 0.600 | t 0.0m\n",
      "Episode 90 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.016 | alpha 0.600 | t 0.0m\n",
      "Episode 100 | Reward -2.0 | Mean(10) -2.0 | v_pol 0.010 | alpha 0.600 | t 0.0m\n",
      "\n",
      "Modelo guardado como FrozenLake-v1_a0.600_g0.800_p0.010_e100_qtable.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================ Lunar-Lander ================\n",
    "\n",
    "# Storing variables\n",
    "\n",
    "# Action, reward and next state matrix [i][j] (i -> Episode // j -> Step on that episode)\n",
    "ev_act = []\n",
    "ev_reward = []\n",
    "ev_state = []\n",
    "\n",
    "# Epsilon and accumulated reward matrix (length = episodes)\n",
    "ev_epsilon = []\n",
    "acc_reward = []\n",
    "\n",
    "envtotal = 1\n",
    "if environment > 1:\n",
    "    envtotal = 2\n",
    "\n",
    "\n",
    "for envnumber in range(envtotal):\n",
    "\n",
    "    # Environment initialization (TODO: Need some Tweaking to support varied environments)\n",
    "    if envnumber < 1:\n",
    "        if environment < 1:\n",
    "            env = gym.make(envname,desc=None,map_name=\"4x4\",is_slippery=False,reward_schedule=(10, -1, -1),render_mode=None)\n",
    "        else:\n",
    "            env = gym.make(envname,render_mode=None)\n",
    "    else:\n",
    "        env = gym.make(envname,render_mode=None)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        for gamma in gammas:\n",
    "            for v_policyI in v_policies:\n",
    "                for episodes in max_episodes:\n",
    "\n",
    "                    v_policy = copy(v_policyI)\n",
    "\n",
    "                    best_mean = -np.inf\n",
    "\n",
    "                    # Q-table first initialization\n",
    "                    if environment < 1:\n",
    "                        Q = (-1)*np.ones([env.observation_space.n, env.action_space.n])\n",
    "                    else:\n",
    "                        Q = np.zeros([bins+1] * 6 + [2, 2] + [env.action_space.n])\n",
    "\n",
    "                    best_Q = None\n",
    "\n",
    "                    if environment < 1:\n",
    "                        max_steps = 20  # Maximum steps allowed before terminating episode no matter what\n",
    "\n",
    "                    mean_recent = -200\n",
    "\n",
    "                    # Flag and episode number when objective is reached for first time\n",
    "                    objective_reached = False\n",
    "                    ep_ob_reached = -1\n",
    "\n",
    "                    ev_epsilon.append([])\n",
    "                    acc_reward.append([])\n",
    "                    if environment < 1:\n",
    "                        ev_act.append([])\n",
    "                        ev_reward.append([])\n",
    "                        ev_state.append([])\n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # print(alpha,gamma,v_policy,episodes)\n",
    "\n",
    "                    # Training phase\n",
    "                    for episode in range(1,episodes+1):\n",
    "                        # break\n",
    "\n",
    "                        # Environment reset\n",
    "                        state, _ =env.reset()\n",
    "\n",
    "                        # Prepare store data for this episode (epsilon only used for Q-learning + ep_greedy)\n",
    "                        if environment < 1:\n",
    "                            ev_state[-1].append([])\n",
    "                            ev_act[-1].append([])\n",
    "                            ev_reward[-1].append([])\n",
    "                            steps = 0\n",
    "                        else:\n",
    "                            state = discretize(state)\n",
    "\n",
    "                        ev_epsilon[-1].append(epsilon)\n",
    "                        acc_reward[-1].append(0.0)\n",
    "\n",
    "                        # Set termination flags\n",
    "                        terminated = False\n",
    "                        truncated = False  # NOT USED\n",
    "    \n",
    "                        if algorithm >= 2:\n",
    "                            act = select_action(state,policy,v_policy)\n",
    "    \n",
    "    \n",
    "                        # Episode simulation\n",
    "                        while not (terminated):\n",
    "\n",
    "\n",
    "                            # Action calculation (not SARSA)\n",
    "                            if algorithm < 2:\n",
    "                                act = select_action(state,policy,v_policy[0])\n",
    "\n",
    "\n",
    "                            # Step execution\n",
    "                            next_state,reward,terminated,truncated,_ = env.step(act)\n",
    "        \n",
    "                            if environment > 1:\n",
    "                                next_state = discretize(next_state)\n",
    "        \n",
    "\n",
    "                            # Next best action calculation (Motecarlo and Q-learning only)\n",
    "                            if algorithm < 2:\n",
    "                                next_act = np.argmax(Q[next_state])\n",
    "                            # Next action calculation (SARSA)\n",
    "                            else:\n",
    "                                next_act = select_action(state,policy,v_policy[0])\n",
    "\n",
    "\n",
    "                            # Q-table and state update\n",
    "                            Q[state][act] += alpha * (reward + gamma * Q[next_state][next_act] - Q[state][act])\n",
    "                            state = next_state\n",
    "        \n",
    "                            # Save all gathered step info\n",
    "                            if environment < 1:\n",
    "                                ev_act [-1][episode-1].append(act)\n",
    "                                ev_reward[-1][episode-1].append(reward)\n",
    "                                ev_state[-1][episode-1].append(state)\n",
    "                            acc_reward[-1][episode-1] += reward\n",
    "\n",
    "\n",
    "                            # Start to erode epsilon value once target is reached for first time (Q-learning+ep_greedy only)\n",
    "                            if mean_recent >= -150.0 and not objective_reached:\n",
    "                                objective_reached = True\n",
    "                                ep_ob_reached = episode\n",
    "        \n",
    "                            # End condition, so each episode doesn't take absurdly long\n",
    "                            if environment < 1:\n",
    "                                if steps >= max_steps:\n",
    "                                    terminated = True\n",
    "                                steps += 1\n",
    "\n",
    "                        if objective_reached and algorithm < 2:\n",
    "                            v_policy[0] = max(v_policy[1],v_policy[0]*v_policy[2])\n",
    "        \n",
    "                        # Estadísticas y parada temprana por media móvil\n",
    "                        if len(acc_reward[-1]) >= window:\n",
    "                            mean_recent = np.mean(acc_reward[-1][-window:])\n",
    "                            if mean_recent > best_mean:\n",
    "                                best_mean = mean_recent\n",
    "                                best_Q = Q.copy()\n",
    "                            if mean_recent >= target_mean_reward:\n",
    "                                print(f\"Parada temprana en episodio {episode}: media {mean_recent:.2f}\")\n",
    "                                break\n",
    "\n",
    "                        if episode % window == 0:\n",
    "                            mean_recent = np.mean(acc_reward[-1][-min(len(acc_reward[-1]), window):])\n",
    "                            elapsed = time.time() - start_time\n",
    "                            print(\n",
    "                                f\"Episode {episode} | Reward {acc_reward[-1][episode-1]:.1f} | Mean({min(len(acc_reward[-1]), window)}) \"\n",
    "                                f\"{mean_recent:.1f} | v_pol {v_policy[0]:.3f} | alpha {alpha:.3f} | t {elapsed/60:.1f}m\"\n",
    "                            )\n",
    "\n",
    "                    # Guardar el mejor modelo disponible\n",
    "                    to_save = best_Q if best_Q is not None else Q\n",
    "                    if not os.path.exists(\"qtables\"):\n",
    "                        os.mkdir(\"qtables\")\n",
    "\n",
    "                    with open(f\"qtables/{envname}_a{alpha:.3f}_g{gamma:.3f}_p{v_policy[0]:.3f}_e{episodes}_qtable.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(to_save, f)\n",
    "                    print(f\"\\nModelo guardado como {envname}_a{alpha:.3f}_g{gamma:.3f}_p{v_policy[0]:.3f}_e{episodes}_qtable.pkl\\n\")\n",
    "\n",
    "                env.close() # Close environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942bf3f",
   "metadata": {},
   "source": [
    "# Visualización de resultados\n",
    "\n",
    "Una vez te has familiarizado con el entorno, el siguiente paso es analizar y visualizar el comportamiento del agente durante un episodio, así como durante todo el entrenamiento. Para ello, deberás crear nuevas variables que permitan guardar la información. \n",
    "\n",
    "Ten en cuenta que entre los aspectos que vas a querer visualizar son: \n",
    "\n",
    "- Trayectoria completa del episodio, mostrando las secuencias de estados y acciones.\n",
    "\n",
    "- Recompensa inmediata obtenida en cada paso.\n",
    "\n",
    "- Recompensa acumulada a lo largo del episodio.\n",
    "\n",
    "- Recompensa media por paso o por episodio.\n",
    "\n",
    "- Detección de estados terminales y las condiciones que los producen.\n",
    "\n",
    "- Evolución temporal de las observaciones y variables relevantes del entorno.\n",
    "\n",
    "Es posible que quieras crear alguna funcion predeterminada (política determinista) que te permita simular ciertas situaciones que quieres poder visualizar, como por ejemplo la detección de estados terminales y las condiciones que los producen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7882f83-a897-49f1-8daa-521225111fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Episode: 0 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 1 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 2 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 3 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:10  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:12  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -13.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 4 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -10.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 5 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:11  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:12  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:13  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:14  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:15  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -16.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 6 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 7 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -9.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 8 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 9 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 10 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 11 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 12 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 13 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 14 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 15 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 16 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:09  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:12  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:13  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:14  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:15  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:16  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:17  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -18.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 17 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 18 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 19 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 20 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 21 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 22 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:10  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:12  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:13  //  Action: 2  //  Reward: -1  // Next_state: 10\n",
      "Step:14  //  Action: 0  //  Reward: -1  // Next_state: 11\n",
      "\n",
      "Accumulated_reward: -15.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 11\n",
      "\n",
      "============================\n",
      "Episode: 23 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 24 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 25 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 26 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:10  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:11  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -12.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 27 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 28 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 29 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 30 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 31 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 32 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 33 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 34 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 35 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 36 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 37 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 38 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 39 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 40 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 41 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 42 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 43 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 44 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:08  //  Action: 2  //  Reward: -1  // Next_state: 10\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 11\n",
      "\n",
      "Accumulated_reward: -10.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 11\n",
      "\n",
      "============================\n",
      "Episode: 45 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 46 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 47 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -9.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 48 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 49 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -9.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 50 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 6\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 51 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 52 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 53 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 54 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 55 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 56 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 57 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 58 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 59 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 60 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 61 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 62 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 63 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 10\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:10  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -11.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 64 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 65 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 66 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 67 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 68 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 69 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 70 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 71 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 72 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 73 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 74 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 75 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 76 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 14\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:10  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:11  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:12  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:13  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -14.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 77 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 78 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 79 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:08  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:10  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -11.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 80 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 81 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 82 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 83 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:09  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:10  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:11  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -12.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 84 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 85 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 14\n",
      "Step:11  //  Action: 1  //  Reward: 10  // Next_state: 15\n",
      "\n",
      "Accumulated_reward: -1.0  //  Mean_reward: -0.08333333333333333\n",
      "Episode reached objective\n",
      "\n",
      "============================\n",
      "Episode: 86 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 87 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 88 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:07  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 89 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 90 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 91 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:09  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:10  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -12.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 92 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 93 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 94 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 95 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 96 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 97 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 98 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 99 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Mean accumulated reward: -5.45\n",
      "Episode_objective_reached: 85\n",
      "============================\n",
      "\n",
      "============================\n",
      "Best_episode: 85 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 14\n",
      "Step:11  //  Action: 1  //  Reward: 10  // Next_state: 15\n",
      "\n",
      "Accumulated_reward: -1.0  //  Mean_reward: -0.08333333333333333 // Objective_reached: 85\n"
     ]
    }
   ],
   "source": [
    "# ============ FROZEN_LAKE =================\n",
    "\n",
    "t_acc_reward = 0\n",
    "best_acc_reward = acc_reward[0]\n",
    "best_data = [ev_act[0], ev_reward[0], ev_state[0]]\n",
    "best_ep_info = [0,ev_epsilon[0]]\n",
    "\n",
    "\n",
    "for i in range(len(ev_state)):\n",
    "    print(f'\\n============================\\nEpisode: {i} // Epsilon: {ev_epsilon[i]}\\n============================')\n",
    "    for j in range(len(ev_state[i])):\n",
    "        s = ''\n",
    "        if j < 10:\n",
    "            s = '0'\n",
    "        print(\"Step:\" + s + f'{j}  //  Action: {ev_act[i][j]}  //  Reward: {ev_reward[i][j]}  // Next_state: {ev_state[i][j]}')\n",
    "    \n",
    "    print(f'\\nAccumulated_reward: {acc_reward[i]}  //  Mean_reward: {acc_reward[i]/(j+1)}')\n",
    "    \n",
    "    if acc_reward[i] > best_acc_reward:\n",
    "        best_acc_reward = acc_reward[i]\n",
    "        best_data = [ev_act[i], ev_reward[i], ev_state[i]]\n",
    "        best_ep_info = [i, ev_epsilon[i]]\n",
    "    \n",
    "    if j == max_steps-1:\n",
    "        print(f'Episode ended because it took too long')\n",
    "    elif ev_state[i][j] != 15:\n",
    "        print(f'Episode ended because terminal state on: {ev_state[i][j]}')\n",
    "    else:\n",
    "        print(f'Episode reached objective')\n",
    "\n",
    "    t_acc_reward += acc_reward[i]\n",
    "    \n",
    "print(f'\\n============================\\nMean accumulated reward: {t_acc_reward/(i+1)}')\n",
    "print(f'Episode_objective_reached: {ep_ob_reached}\\n============================')\n",
    "\n",
    "print(f'\\n============================\\nBest_episode: {best_ep_info[0]} // Epsilon: {best_ep_info[1]}\\n============================')\n",
    "for j in range(len(best_data[0])):\n",
    "    s = ''\n",
    "    if j < 10:\n",
    "        s = '0'\n",
    "    print(\"Step:\" + s + f'{j}  //  Action: {best_data[0][j]}  //  Reward: {best_data[1][j]}  // Next_state: {best_data[2][j]}')\n",
    "    \n",
    "print(f'\\nAccumulated_reward: {best_acc_reward}  //  Mean_reward: {best_acc_reward/(j+1)} // Objective_reached: {ep_ob_reached}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13441f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(acc_reward, label=\"Recompensa por episodio\", alpha=0.7)\n",
    "\n",
    "if len(acc_reward) > window:\n",
    "    mv = np.convolve(acc_reward, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, window-1+len(mv)), mv, label=f\"Media móvil ({window})\")\n",
    "\n",
    "plt.xlabel(\"Episodios\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.title(\"Aprendizaje con Q-Learning - LunarLander\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead854cc",
   "metadata": {},
   "source": [
    "# Recompensas personalizadas\n",
    "\n",
    "Aunque el entorno Frozen Lake tiene un parámetro para indicar recompensas cuando se producen ciertos estados (alcanzar la meta, caerse en un agujero, o llegar a un bloque de hielo), es posible que queramos definir otras recompensas dado un cierto estado. Por ejemplo, penalizar una acción que no produce un cambio de estado (en este caso seria intentar salirse del mapa). Para ello es necesario implementar nuestro propio \"wrapper\" de recompensas. Aunque para el caso que hemos planteado, en lugar de realizar un wrapper sobre la recompensa, es posible aplicarlo de la misma manera sobre el método step.\n",
    "\n",
    "La última versión de Gymnasium te permite hacerlo. Aquí puedes consultar la información: [Documentación RewardWrapper](https://gymnasium.farama.org/tutorials/gymnasium_basics/implementing_custom_wrappers/#inheriting-from-gymnasium-rewardwrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class FrozenLakePenaltyWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "\n",
    "    #Para este caso, debes de wrappear el metodo step\n",
    "    def step(self, action):\n",
    "        #Falta código\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13793e45-035e-40e8-94f5-acae7abcf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el entorno FrozenLake indicandole los parámetros. Para empezar, que el parámetro is_slippery = False.\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, map_name=\"4x4\", reward_schedule=(10,0,0), render_mode=\"human\")\n",
    "\n",
    "# Aplicar el wrapper\n",
    "env = #Falta código\n",
    "\n",
    "\n",
    "##Define el número de episodios y comprueba que el wrapper funciona ;) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8bd19-caf5-4ce0-99e6-80ab2e9c6115",
   "metadata": {},
   "source": [
    "# Acciones personalizadas\n",
    "\n",
    "Similarmente a la personalización de las recompensas, podemos personalizar las acciones que se realizan en el entorno. FrozenLake tiene el modo \"is_slippery\" que básicamente lo simula repite una acción con una probabilidad dado un estado (si es bloque de hielo).\n",
    "\n",
    "Sin embargo, otra de las funcionalidades útiles que tiene, es que permite discretizar un conjunto de acciones continuo. Consulta la documentación e prueba el ejemplo que aparece en la documentación para entenderlo. Aquí puedes consultar la información: [Documentación ActionWrapper](https://gymnasium.farama.org/tutorials/gymnasium_basics/implementing_custom_wrappers/#inheriting-from-gymnasium-actionwrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1569840-9c40-4384-8acb-c744667e018b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a232cca-39ff-4148-bb4e-6d867fa0a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mani",
   "language": "python",
   "name": "mani"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
