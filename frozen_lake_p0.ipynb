{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc233d8-6e79-4ec7-b24a-7cd06efa5333",
   "metadata": {},
   "source": [
    "# Práctica 0: Frozen Lake\n",
    "\n",
    "En este notebook trabajaremos con el entorno **Frozen Lake**, parte de la librería OpenAI Gym. Este entorno simula un lago congelado donde un agente debe aprender a llegar a su objetivo (la meta) mientras evita caer en agujeros en el hielo. Es un problema clásico de **aprendizaje por refuerzo** (Reinforcement Learning) que ilustra conceptos como:\n",
    "\n",
    "- Modelado de entornos utilizando **procesos de decisión de Markov (MDPs)**.\n",
    "- Definición de políticas y recompensas.\n",
    "- Uso de algoritmos de planificación como **Value Iteration** y **Policy Iteration**.\n",
    "\n",
    "## Descripción del entorno\n",
    "El entorno de Frozen Lake consiste en una cuadrícula de 4x4 donde cada celda puede representar uno de los siguientes estados:\n",
    "- **S** (Start): La posición inicial del agente.\n",
    "- **F** (Frozen): Una celda segura que el agente puede atravesar.\n",
    "- **H** (Hole): Un agujero en el que el agente caerá, terminando el episodio.\n",
    "- **G** (Goal): El objetivo que el agente debe alcanzar.\n",
    "\n",
    "![Escenario](https://www.gymlibrary.dev/_images/frozen_lake.gif)\n",
    "\n",
    "El agente puede moverse (acciones) en cuatro direcciones: **arriba**, **abajo**, **izquierda**, y **derecha**. Aunque el ejemplo clásico incluye el efecto de hielo resbaladizo, lo que introduce un grado de aleatoriedad en el entorno, en este ejercicio y con el objetivo de centrarnos en otros aspectos, utilizaremos la versión sin Sin embargo, debido al hielo resbaladizo, los movimientos no siempre son precisos, lo que introduce un grado de aleatoriedad en el entorno. Sin embargo, por simplificar el ejercicio, no incluiremos la parte resbaladiza.\n",
    "\n",
    "En caso de que alguno de los acciones haga que el agente salga del mapa, la acción no se llevará a cabo (y su estado no cambiará)\n",
    "\n",
    "### Acciones\n",
    "\n",
    "El agente utilizará un vector de 1 elemento para las acciones que puede realizar. En el ejercicio de **FrozenLake**, el espacio de acciones es discreto, y este puede tomar uno de los siguientes valores para decidir la dirección en la que moverse:\n",
    "\n",
    "- 0: IZQUIERDA\n",
    "\n",
    "- 1: ABAJO\n",
    "\n",
    "- 2: DERECHA\n",
    "\n",
    "- 3: ARRIBA\n",
    "\n",
    "### Estados (o espacio de observación)\n",
    "\n",
    "El modelado de estados es una parte esencial a la hora de resolver los problemas siguiendo el proceso de decisión de Markov (MDP). El estado representa la posición actual del agente. El número de estados posibles depende del mapa en este caso. Dado que tenemos un problema discreto, donde los estados posibles son 16 (uno para cada posición en la rejilla 4x4), el estado (o posición actual) del agente puede calcularse como *fila actual x n_filas + columna actual*, donde tanto fila como columna empiezan en 0. Por ejemplo, la posicion inicial será el estado 0 (*estado = 0 x 4 + 0 = 0*). El estado final puede calcularse de la siguiente manera (*estado = 3 x 4 + 3 = 15*). O el bloque de hielo de la ultima fila seria (*estado = 3 x 4 + 0 = 12*) \n",
    "\n",
    "\n",
    "## Objetivo\n",
    "El objetivo del ejercicio es implementar diferentes estrategias de aprendizaje para entrenar al agente a maximizar su probabilidad de alcanzar la meta sin caer en los agujeros.\n",
    "\n",
    "Más en detalle, vamos a implementar diferentes politicas y algoritmos de aprendizaje por refuerzo.\n",
    "\n",
    "1. **Montecarlo**\n",
    "2. **Q-Learning**\n",
    "\n",
    "Además, utilizaremos para cada uno de los algoritmos diferentes politicas:\n",
    "\n",
    "1. **Greedy**\n",
    "2. **Epsilon-Greedy**\n",
    "3. **SOFTMAX**\n",
    "\n",
    "### Herramientas necesarias\n",
    "Este notebook utilizará:\n",
    "- **Python**: Para implementar el código.\n",
    "- **OpenAI Gym**: Para simular el entorno.\n",
    "- **Numpy**: Para cálculos numéricos.\n",
    "- **Matplotlib** (opcional): En caso de que quieras visualizar los resultados con más detalle.\n",
    "\n",
    "Para simplificar la configuración, y evitar problemas de versiones y configuraciones, tienes en el PDF adjunto de la práctica los pasos para instalar las librerias necesarias. \n",
    "\n",
    "Ahora si, con todo listo y sabiendo que tenemos que implementar, ¡comencemos a explorar Frozen Lake!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a286a5",
   "metadata": {},
   "source": [
    "Lo primero es importar, las librerias que vamos a utilizar. El entorno gym ya importa algunos paquetes cuando lo invocamos (como pygame) que es lo que nos permite visualizar el agente y el entorno. En este sentido, será suficiente con que importemos las librerias de Gymnasium y Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6968dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import RewardWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85687c",
   "metadata": {},
   "source": [
    "# Creación del entorno en Gym\n",
    "Ahora vamos a crear el entorno haciendo uso de la función **gym.make()** \n",
    "\n",
    "[Documentación de frozen lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "- En el primer argumento del método (**id del juego**) le indicaremos el entorno que vamos a cargar, en este caso sera *FrozenLake-v1*. Si quisiesemos probar en otros entornos (aunque no es objetivo de la práctica), cambiariamos la versión o el nombre en este parámetro.\n",
    "\n",
    "- El segundo argumento **desc**, no lo vamos a utilizar (por simplicidad), pero basicamente nos permitiría definir nuestro propio entorno de obstaculos y bloques de hielo en lugar del entorno por defecto que vamos a utilizar.\n",
    "\n",
    "- Otro parametro importante es **map_name**, y depende del entorno. En el caso del juego *FrozenLake*, este puede obtener los valore \"4x4\" o \"8x8\". En nuestro caso, elegiremos el entorno 4x4, para facilitar la convergencia de los algoritmos.\n",
    "\n",
    "- **is_slippery** es un parámetro especifico del juego FrozenLake. Si el valor de este parámetro fuese True, lo que haría es aplicar un factor de \"resbalamiento\" o probabilidad de resbalar, que con una probabilidad 1/3 repetiria la acción anterior (simulando que el agente resbala y mantiene la dirección). Tal y como indicabamos previamente, no vamos a incluir este factor de aleatoriedad y pondremos el valor a False. Se usa en conjunción con el parámetro **success_rate** para indicar la probabilidad de resbalar en un bloque de hielo.\n",
    "\n",
    "- **reward_schedule** es el parámetro que permite modificar las recompensas obtenidas por el entorno. Como podras ver en la documentación, por defecto recibe (1, 0, 0), que son las recompensas que recibe el agente cuando alcanza la emta, alcanza un agujero, o alcanza un bloque de hielo respectivamente.\n",
    "\n",
    "- El último parámetro, que no aparece en la documentación es **render_mode**. Aunque se podria visualizar mediante una matriz que indique el estado (realmente se representa así), vamos a utilizar el modo \"human\" el cual usa pygame, y nos permitira ver el comportamiento del agente por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff256cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el entorno FrozenLake indicandole los parámetros. Para empezar, que el parámetro is_slippery = False.\n",
    "env = gym.make('FrozenLake-v1',\n",
    "               desc=None,\n",
    "               map_name=\"4x4\",\n",
    "               is_slippery=False,\n",
    "               reward_schedule=(1, 0, 0),\n",
    "               render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2da10",
   "metadata": {},
   "source": [
    "# Exploración del entorno\n",
    "\n",
    "Una vez creado el entorno (env), gym nos provee de algunas funciones para explorar el entorno creado. En concreto, podemos invocar los métodos *env.obervation_space* y *env.action_space*. Ambos, permiten invocar el \"método\" n, que nos indica el tamaño del espacio de accion o de observación y el metodo \"sample()\" que seleccionaria de manera aleatoria un elemento del espacio (ya sea de accion o de observación). Visualiza por pantalla (con un print) el tamaño y una muestra aleatoria de cada uno de los espacios (de acciones y de observación). Como puedes comprobar (si ejecutas varias veces), el tamaño o es espacio no cambia, pero si la muestra elegida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9dabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espacio de observación\n",
      "\n",
      "Tamaño del espacio de observación: 16\n",
      "Estado del espacio de observación: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Espacio de observación\\n\")\n",
    "print(\"Tamaño del espacio de observación:\", env.observation_space.n)\n",
    "print(\"Estado del espacio de observación:\", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e052d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espacio de acciones\n",
      "Tamaño del espacio de acciones: 4\n",
      "Estado del espacio de observación: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Espacio de acciones\")\n",
    "print(\"Tamaño del espacio de acciones:\", env.action_space.n)\n",
    "print(\"Estado del espacio de observación:\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb508efe",
   "metadata": {},
   "source": [
    "# Ejecución simple del entorno FrozenLake\n",
    "\n",
    "Antes de empezar a programar políticas y algoritmos, vamos a comprobar que todo nos funciona correctamente. Para ello, vamos a ejecutar el entorno realizando movimientos aleatorios. Además, puedes programar tambien tu propio episodio donde el agente realice las acciones predeterminadas que determine.\n",
    "\n",
    "En el siguiente [Enlace](https://gymnasium.farama.org/api/env/) puedes ver los métodos más importantes para interactuar con el entorno. A modo de resumen, el **método reset** nos permite reiniciar el entorno entre diferentes episodios(ya que tras un episodio queremos volver a comenzar en la posición inicial). El **método render**, ya lo utilizamos al principio, pero nos permite visualizar lo que el agente hace (en este caso sera human ya que lo definimos previamente) aunque más adelante usaremos otros modos. Y por último, el método más importante es el método **step** ya que es el método que nos va a permitir la transicción de estado (dada una acción).\n",
    "\n",
    "Los pseudopasos a grandes rasgos (aunque te ayudo con los comentarios en la siguiente celda) son:\n",
    "\n",
    "1. Define el número de episodios\n",
    "2. Para cada episodio debemos de realizar una serie de pasos:\n",
    "   - Reinicia el entorno mediante el método reset.\n",
    "   - Mientras no hayamos alcanzado un estado terminal (esto se produce cuando alcanza la meta, o cae en un bloque de hielo)... \n",
    "        - Renderiza\n",
    "        - Elige una acción (en este caso elegiremos una acción aleatoria o predeterminada). Para el caso de la acción aleatoria, es preferible definir un método que realice dicha acción. En un futuro cambiaremos la politica ;) \n",
    "        - Ejecuta un step (ten en cuenta las variables que devuelve para que te funcione ;) )\n",
    "3. Recuerda cerrar el entorno mediante el **método env.close()** tras finalizar TODOS los episodios\n",
    "\n",
    "Tras implementar estos pasos correctamente, veras el tablero y como el agente interactua con el entorno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f561411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mani/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 3\n",
      "0 4 1\n",
      "0 0 3\n",
      "0 4 1\n",
      "0 8 1\n",
      "0 12 1\n",
      "1 0 3\n",
      "1 0 3\n",
      "1 1 2\n",
      "1 5 1\n"
     ]
    }
   ],
   "source": [
    "# Variables para almacenar la información de cada episodio\n",
    "\n",
    "\n",
    "#Define el número de episodios (2 es un buen número para empezar)\n",
    "episodes = 2\n",
    "for episode in range(episodes):\n",
    "    # Reiniciar el entorno\n",
    "    act, _ =env.reset()\n",
    "\n",
    "    # Simular un episodio hasta que llege.\n",
    "    terminated = False\n",
    "    while not (terminated):\n",
    "\n",
    "        # Elegir una acción aleatoria\n",
    "        act = env.action_space.sample()\n",
    "        \n",
    "        #Renderización del entorno para visualizar lo que el agente hace\n",
    "        env.render()\n",
    "\n",
    "        #Ejecutamos un step\n",
    "        observation,reward,terminated,truncated,_ = env.step(act)\n",
    "        \n",
    "        # Actualizar el estado\n",
    "        print(episode,observation,act)\n",
    "        \n",
    "#Cerramos el entorno\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15aea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic RL parameters\n",
    "alpha = 0.3   # Learning rate parameter\n",
    "gamma = 0.95  # Discount parameter\n",
    "\n",
    "# Erode parameter for epsilon greedy (enacted each episode once goal is reached)\n",
    "l = 0.97\n",
    "\n",
    "# Q-table first initialization\n",
    "Q = (-1)*np.ones([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Action policies\n",
    "def epsilon_greedy(state):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "# Q[0][3] = 2.0\n",
    "# print(epsilon_greedy(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cea9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-LEARNING ALGORITHM (EPSILON_GREEDY)\n",
    "\n",
    "# Storing variables\n",
    "\n",
    "# Action, reward and next state matrix [i][j] (i -> Episode // j -> Step on that episode)\n",
    "ev_act = []\n",
    "ev_reward = []\n",
    "ev_state = []\n",
    "\n",
    "# Epsilon and accumulated reward matrix (length = episodes)\n",
    "ev_epsilon = []\n",
    "acc_reward = []\n",
    "\n",
    "# Environment initialization\n",
    "env = gym.make('FrozenLake-v1',desc=None,map_name=\"4x4\",is_slippery=False,reward_schedule=(10, -1, -1))\n",
    "\n",
    "\n",
    "epsilon = 0.999 # Starting (Q-learning only) epsilon value\n",
    "\n",
    "episodes = 100  # Total episodes of the training\n",
    "max_steps = 20  # Maximum steps allowed before terminating episode no matter what\n",
    "\n",
    "# Flag and episode number when objective is reached for first time\n",
    "objective_reached = False\n",
    "ep_ob_reached = -1\n",
    "\n",
    "# Training phase\n",
    "for episode in range(episodes):\n",
    "    # Environment reset\n",
    "    act, _ =env.reset()\n",
    "    state = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Prepare store data for this episode (epsilon only used for Q-learning + ep_greedy)\n",
    "    ev_state.append([])\n",
    "    ev_act.append([])\n",
    "    ev_reward.append([])\n",
    "    ev_epsilon.append(epsilon)\n",
    "    acc_reward.append(0.0)\n",
    "\n",
    "    # Set termination flags\n",
    "    terminated = False\n",
    "    truncated = False  # NOT USED\n",
    "    \n",
    "    # Episode simulation\n",
    "    while not (terminated):\n",
    "\n",
    "\n",
    "        # Action calculation (SARSA)\n",
    "        act = epsilon_greedy(state)\n",
    "\n",
    "\n",
    "        # Step execution\n",
    "        next_state,reward,terminated,truncated,_ = env.step(act)\n",
    "        \n",
    "\n",
    "        # Next best action calculation (Motecarlo and Q-learning only)\n",
    "        best = np.argmax(Q[next_state])\n",
    "\n",
    "        # Q-table and state update\n",
    "        Q[state][act] += alpha * (reward + gamma * Q[next_state][best] - Q[state][act])\n",
    "        state = next_state\n",
    "        \n",
    "        # Save all gathered step info\n",
    "        ev_act[episode].append(act)\n",
    "        ev_reward[episode].append(reward)\n",
    "        ev_state[episode].append(state)\n",
    "        acc_reward[episode] += reward\n",
    "\n",
    "\n",
    "        # Start to erode epsilon value once target is reached for first time (Q-learning+ep_greedy only)\n",
    "        if reward > 0.0 and not objective_reached:\n",
    "            objective_reached = True\n",
    "            ep_ob_reached = episode\n",
    "        \n",
    "        # End condition, so each episode doesn't take absurdly long\n",
    "        if steps >= max_steps:\n",
    "            terminated = True\n",
    "        steps += 1\n",
    "    \n",
    "    if objective_reached:\n",
    "        epsilon *= l\n",
    "\n",
    "env.close() # Close environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6df59702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA ALGORITHM (EPSILON GREEDY)\n",
    "\n",
    "# Storing variables\n",
    "\n",
    "# Action, reward and next state matrix [i][j] (i -> Episode // j -> Step on that episode)\n",
    "ev_act = []\n",
    "ev_reward = []\n",
    "ev_state = []\n",
    "\n",
    "# Epsilon and accumulated reward matrix (length = episodes)\n",
    "ev_epsilon = [] \n",
    "acc_reward = []\n",
    "\n",
    "# Environment initialization\n",
    "env = gym.make('FrozenLake-v1',desc=None,map_name=\"4x4\",is_slippery=False,reward_schedule=(10, -1, -1))\n",
    "\n",
    "\n",
    "epsilon = 0.5   # Permanent (SARSA only) epsilon value\n",
    "\n",
    "episodes = 100  # Total episodes of the training\n",
    "max_steps = 20  # Maximum steps allowed before terminating episode no matter what\n",
    "\n",
    "# Flag and episode number when objective is reached for first time\n",
    "objective_reached = False\n",
    "ep_ob_reached = -1\n",
    "\n",
    "# Training phase\n",
    "for episode in range(episodes):\n",
    "    # Environment reset\n",
    "    act, _ =env.reset()\n",
    "    state = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Prepare store data for this episode (epsilon only used for Q-learning + ep_greedy)\n",
    "    ev_act.append([])\n",
    "    ev_reward.append([])\n",
    "    ev_state.append([])\n",
    "    ev_epsilon.append(epsilon)\n",
    "    acc_reward.append(0.0)\n",
    "    \n",
    "    # Set termination flags\n",
    "    terminated = False\n",
    "    truncated = False  # NOT USED\n",
    "\n",
    "    # First action (only used with SARSA)\n",
    "    act = epsilon_greedy(state)\n",
    "    \n",
    "    # Episode simulation\n",
    "    while not (terminated):\n",
    "\n",
    "        # Step execution\n",
    "        next_state,reward,terminated,truncated,_ = env.step(act)\n",
    "\n",
    "        # Next action calculation (SARSA)\n",
    "        next_act = epsilon_greedy(state)\n",
    "        \n",
    "        # Q-table and state update (also action for SARSA)\n",
    "        Q[state][act] += alpha * (reward + gamma * Q[next_state][next_act] - Q[state][act])\n",
    "        state = next_state\n",
    "        act = next_act\n",
    "        \n",
    "        # Save all gathered step info\n",
    "        ev_act[episode].append(act)\n",
    "        ev_reward[episode].append(reward)\n",
    "        ev_state[episode].append(state)\n",
    "        acc_reward[episode] += reward\n",
    "\n",
    "\n",
    "        # Start to erode epsilon value once target is reached for first time (Q-learning+ep_greedy only)\n",
    "        if reward > 0.0 and not objective_reached:\n",
    "            objective_reached = True\n",
    "            ep_ob_reached = episode\n",
    "        \n",
    "        # End condition, so each episode doesn't take absurdly long\n",
    "        if steps >= max_steps:\n",
    "            terminated = True\n",
    "        steps += 1\n",
    "\n",
    "env.close() # Close environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942bf3f",
   "metadata": {},
   "source": [
    "# Visualización de resultados\n",
    "\n",
    "Una vez te has familiarizado con el entorno, el siguiente paso es analizar y visualizar el comportamiento del agente durante un episodio, así como durante todo el entrenamiento. Para ello, deberás crear nuevas variables que permitan guardar la información. \n",
    "\n",
    "Ten en cuenta que entre los aspectos que vas a querer visualizar son: \n",
    "\n",
    "- Trayectoria completa del episodio, mostrando las secuencias de estados y acciones.\n",
    "\n",
    "- Recompensa inmediata obtenida en cada paso.\n",
    "\n",
    "- Recompensa acumulada a lo largo del episodio.\n",
    "\n",
    "- Recompensa media por paso o por episodio.\n",
    "\n",
    "- Detección de estados terminales y las condiciones que los producen.\n",
    "\n",
    "- Evolución temporal de las observaciones y variables relevantes del entorno.\n",
    "\n",
    "Es posible que quieras crear alguna funcion predeterminada (política determinista) que te permita simular ciertas situaciones que quieres poder visualizar, como por ejemplo la detección de estados terminales y las condiciones que los producen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7882f83-a897-49f1-8daa-521225111fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Episode: 0 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 1 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 2 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 3 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:10  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:12  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -13.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 4 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -10.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 5 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:11  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:12  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:13  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:14  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:15  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -16.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 6 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 7 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -9.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 8 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 9 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 10 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 11 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 12 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 13 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 14 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 15 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 16 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:09  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:12  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:13  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:14  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:15  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:16  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:17  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -18.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 17 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 18 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 19 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 20 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 21 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 22 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:10  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:12  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:13  //  Action: 2  //  Reward: -1  // Next_state: 10\n",
      "Step:14  //  Action: 0  //  Reward: -1  // Next_state: 11\n",
      "\n",
      "Accumulated_reward: -15.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 11\n",
      "\n",
      "============================\n",
      "Episode: 23 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 24 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 25 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 26 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:10  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:11  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -12.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 27 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 28 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 29 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 30 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 31 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 32 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 33 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 34 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 35 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 36 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 37 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 38 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 39 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 40 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 41 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 42 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 43 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 44 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:08  //  Action: 2  //  Reward: -1  // Next_state: 10\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 11\n",
      "\n",
      "Accumulated_reward: -10.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 11\n",
      "\n",
      "============================\n",
      "Episode: 45 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 46 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 47 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -9.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 48 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 49 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -9.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 50 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 6\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 51 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 52 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 53 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 3\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 54 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 55 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 56 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 57 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 58 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 59 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 60 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 61 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 62 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 63 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 10\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:10  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -11.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 64 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 65 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 66 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 67 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 68 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 69 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 70 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 71 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 72 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 73 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 74 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 75 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 76 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 14\n",
      "Step:08  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:10  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:11  //  Action: 0  //  Reward: -1  // Next_state: 8\n",
      "Step:12  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:13  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -14.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 77 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 78 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:05  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 79 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 3\n",
      "Step:07  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:08  //  Action: 2  //  Reward: -1  // Next_state: 3\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:10  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -11.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 80 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 81 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 82 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -5.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 83 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 0  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 0\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 4\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:08  //  Action: 1  //  Reward: -1  // Next_state: 6\n",
      "Step:09  //  Action: 0  //  Reward: -1  // Next_state: 10\n",
      "Step:10  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:11  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -12.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 84 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 0  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 85 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 14\n",
      "Step:11  //  Action: 1  //  Reward: 10  // Next_state: 15\n",
      "\n",
      "Accumulated_reward: -1.0  //  Mean_reward: -0.08333333333333333\n",
      "Episode reached objective\n",
      "\n",
      "============================\n",
      "Episode: 86 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:03  //  Action: 3  //  Reward: -1  // Next_state: 2\n",
      "Step:04  //  Action: 2  //  Reward: -1  // Next_state: 2\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 3\n",
      "Step:06  //  Action: 1  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 87 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 88 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:07  //  Action: 0  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -8.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 89 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 13\n",
      "Step:05  //  Action: 3  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 90 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 91 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 13\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 13\n",
      "Step:09  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:10  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:11  //  Action: 1  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -12.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 92 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 93 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 94 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 3  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -6.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 95 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 2\n",
      "Step:02  //  Action: 2  //  Reward: -1  // Next_state: 6\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 7\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 7\n",
      "\n",
      "============================\n",
      "Episode: 96 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 0  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -2.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 97 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 9\n",
      "Step:05  //  Action: 1  //  Reward: -1  // Next_state: 8\n",
      "Step:06  //  Action: 2  //  Reward: -1  // Next_state: 12\n",
      "\n",
      "Accumulated_reward: -7.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 12\n",
      "\n",
      "============================\n",
      "Episode: 98 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 2  //  Reward: -1  // Next_state: 0\n",
      "Step:01  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -4.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Episode: 99 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 3  //  Reward: -1  // Next_state: 1\n",
      "Step:01  //  Action: 1  //  Reward: -1  // Next_state: 1\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 5\n",
      "\n",
      "Accumulated_reward: -3.0  //  Mean_reward: -1.0\n",
      "Episode ended because terminal state on: 5\n",
      "\n",
      "============================\n",
      "Mean accumulated reward: -5.45\n",
      "Episode_objective_reached: 85\n",
      "============================\n",
      "\n",
      "============================\n",
      "Best_episode: 85 // Epsilon: 0.5\n",
      "============================\n",
      "Step:00  //  Action: 1  //  Reward: -1  // Next_state: 4\n",
      "Step:01  //  Action: 2  //  Reward: -1  // Next_state: 8\n",
      "Step:02  //  Action: 1  //  Reward: -1  // Next_state: 9\n",
      "Step:03  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:04  //  Action: 0  //  Reward: -1  // Next_state: 14\n",
      "Step:05  //  Action: 2  //  Reward: -1  // Next_state: 13\n",
      "Step:06  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:07  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:08  //  Action: 3  //  Reward: -1  // Next_state: 14\n",
      "Step:09  //  Action: 1  //  Reward: -1  // Next_state: 10\n",
      "Step:10  //  Action: 2  //  Reward: -1  // Next_state: 14\n",
      "Step:11  //  Action: 1  //  Reward: 10  // Next_state: 15\n",
      "\n",
      "Accumulated_reward: -1.0  //  Mean_reward: -0.08333333333333333 // Objective_reached: 85\n"
     ]
    }
   ],
   "source": [
    "t_acc_reward = 0\n",
    "best_acc_reward = acc_reward[0]\n",
    "best_data = [ev_act[0], ev_reward[0], ev_state[0]]\n",
    "best_ep_info = [0,ev_epsilon[0]]\n",
    "\n",
    "\n",
    "for i in range(len(ev_state)):\n",
    "    print(f'\\n============================\\nEpisode: {i} // Epsilon: {ev_epsilon[i]}\\n============================')\n",
    "    for j in range(len(ev_state[i])):\n",
    "        s = ''\n",
    "        if j < 10:\n",
    "            s = '0'\n",
    "        print(\"Step:\" + s + f'{j}  //  Action: {ev_act[i][j]}  //  Reward: {ev_reward[i][j]}  // Next_state: {ev_state[i][j]}')\n",
    "    \n",
    "    print(f'\\nAccumulated_reward: {acc_reward[i]}  //  Mean_reward: {acc_reward[i]/(j+1)}')\n",
    "    \n",
    "    if acc_reward[i] > best_acc_reward:\n",
    "        best_acc_reward = acc_reward[i]\n",
    "        best_data = [ev_act[i], ev_reward[i], ev_state[i]]\n",
    "        best_ep_info = [i, ev_epsilon[i]]\n",
    "    \n",
    "    if j == max_steps-1:\n",
    "        print(f'Episode ended because it took too long')\n",
    "    elif ev_state[i][j] != 15:\n",
    "        print(f'Episode ended because terminal state on: {ev_state[i][j]}')\n",
    "    else:\n",
    "        print(f'Episode reached objective')\n",
    "\n",
    "    t_acc_reward += acc_reward[i]\n",
    "    \n",
    "print(f'\\n============================\\nMean accumulated reward: {t_acc_reward/(i+1)}')\n",
    "print(f'Episode_objective_reached: {ep_ob_reached}\\n============================')\n",
    "\n",
    "print(f'\\n============================\\nBest_episode: {best_ep_info[0]} // Epsilon: {best_ep_info[1]}\\n============================')\n",
    "for j in range(len(best_data[0])):\n",
    "    s = ''\n",
    "    if j < 10:\n",
    "        s = '0'\n",
    "    print(\"Step:\" + s + f'{j}  //  Action: {best_data[0][j]}  //  Reward: {best_data[1][j]}  // Next_state: {best_data[2][j]}')\n",
    "    \n",
    "print(f'\\nAccumulated_reward: {best_acc_reward}  //  Mean_reward: {best_acc_reward/(j+1)} // Objective_reached: {ep_ob_reached}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead854cc",
   "metadata": {},
   "source": [
    "# Recompensas personalizadas\n",
    "\n",
    "Aunque el entorno Frozen Lake tiene un parámetro para indicar recompensas cuando se producen ciertos estados (alcanzar la meta, caerse en un agujero, o llegar a un bloque de hielo), es posible que queramos definir otras recompensas dado un cierto estado. Por ejemplo, penalizar una acción que no produce un cambio de estado (en este caso seria intentar salirse del mapa). Para ello es necesario implementar nuestro propio \"wrapper\" de recompensas. Aunque para el caso que hemos planteado, en lugar de realizar un wrapper sobre la recompensa, es posible aplicarlo de la misma manera sobre el método step.\n",
    "\n",
    "La última versión de Gymnasium te permite hacerlo. Aquí puedes consultar la información: [Documentación RewardWrapper](https://gymnasium.farama.org/tutorials/gymnasium_basics/implementing_custom_wrappers/#inheriting-from-gymnasium-rewardwrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class FrozenLakePenaltyWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "\n",
    "    #Para este caso, debes de wrappear el metodo step\n",
    "    def step(self, action):\n",
    "        #Falta código\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13793e45-035e-40e8-94f5-acae7abcf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el entorno FrozenLake indicandole los parámetros. Para empezar, que el parámetro is_slippery = False.\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, map_name=\"4x4\", reward_schedule=(10,0,0), render_mode=\"human\")\n",
    "\n",
    "# Aplicar el wrapper\n",
    "env = #Falta código\n",
    "\n",
    "\n",
    "##Define el número de episodios y comprueba que el wrapper funciona ;) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8bd19-caf5-4ce0-99e6-80ab2e9c6115",
   "metadata": {},
   "source": [
    "# Acciones personalizadas\n",
    "\n",
    "Similarmente a la personalización de las recompensas, podemos personalizar las acciones que se realizan en el entorno. FrozenLake tiene el modo \"is_slippery\" que básicamente lo simula repite una acción con una probabilidad dado un estado (si es bloque de hielo).\n",
    "\n",
    "Sin embargo, otra de las funcionalidades útiles que tiene, es que permite discretizar un conjunto de acciones continuo. Consulta la documentación e prueba el ejemplo que aparece en la documentación para entenderlo. Aquí puedes consultar la información: [Documentación ActionWrapper](https://gymnasium.farama.org/tutorials/gymnasium_basics/implementing_custom_wrappers/#inheriting-from-gymnasium-actionwrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1569840-9c40-4384-8acb-c744667e018b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a232cca-39ff-4148-bb4e-6d867fa0a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mani",
   "language": "python",
   "name": "mani"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
